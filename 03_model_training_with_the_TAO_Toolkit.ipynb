{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8416bc8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c19d14",
   "metadata": {},
   "source": [
    "# Model Training with the TAO Toolkit #\n",
    "By way of review, this diagram illustrates is what a typical model development workflow looks like. We started by preparing the data and a pre-trained model. Next, we will train the model with new data and evaluate its performance. We will then export the model once its satisfactory. _Note that this does not include the optimization for inference, which we will explore in the next notebook._\n",
    "\n",
    "<p><img src='images/pre-trained_model_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84deb7-7cd8-49ca-8d46-2cf4fefd1122",
   "metadata": {},
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to use the TAO Toolkit to train a video AI model, including: \n",
    "* Taking the Purpose-Built TrafficCamNet Model and Training a Custom ResNet18 DetectNet_v2 Model for Object Detection\n",
    "* Creating Model Training, Evaluation, and Inference Configuration Files\n",
    "* Evaluating Model\n",
    "* Deploying Model to DeepStream\n",
    "\n",
    "**Table of Contents** <br>\n",
    "This notebook covers the below sections: \n",
    "1. [Model Training](#s1)\n",
    "    * [Exercise #1 - Modify Data Set Config](#e1)\n",
    "    * [Exercise #2 - Modify Augmentation Config](#e2)\n",
    "    * [Exercise #3 - Modify Model Config](#e3)\n",
    "    * [Exercise #4 - Modify BBox Rasterizer Config](#e4)\n",
    "    * [Exercise #5 - Modify Post-Processing Config](#e5)\n",
    "    * [Exercise #6 - Modify Training Config](#e6)\n",
    "    * [Cost Function Config](#s1.1)\n",
    "    * [Exercise #7 - Modify Evaluation Config](#e7)\n",
    "    * [Initiate Model Training](#s1.2)\n",
    "2. [Evaluating the Model](#s2)\n",
    "3. [Model Inference](#s3)\n",
    "    * [Exercise #8 - Modify Inferencer Config](#e8)\n",
    "    * [Exercise #9 - Modify BBox Handler Config](#e9)\n",
    "    * [Visualize Inference](#s3.1)\n",
    "4. [Model Export](#s4)\n",
    "    * [Model Export as TensorRT Engine](#s4.1)\n",
    "5. [Deploying to DeepStream](#s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2853ddb-59f5-443f-80b4-11458f1f0078",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1'></a>\n",
    "## Model Training ##\n",
    "Training configuration is done through a training spec file, which includes options such as which data set to use for training, which data set to use for validation, which pre-trained model architecture to use, which hyperparamters to tune, and other training options. The `train` and `evaluate` subtasks for a DetectNet_v2 experiment share the same configuration file. Configuration files can be created from scratch or modified using the templates provided in TAO Toolkit's [sample applications](https://docs.nvidia.com/tao/tao-toolkit/#cv-applications). \n",
    "\n",
    "<p><img src='images/rewind.png' width=720></p>\n",
    "\n",
    "_Recall that when working with purpose-built models from NGC, the correct **encryption key** is required to load the model. Users will be able to define their own export encryption key when traing from a general purpose model. This is to protect proprietary IP and used to decrypt the `.etlt` model in DeepStream applications._\n",
    "\n",
    "<p><img src='images/encryption_key.png' width=540></p>\n",
    "\n",
    "The training configuration file has eight sections: \n",
    "* `dataset_config`\n",
    "* `augmentation_config`\n",
    "* `model_config`\n",
    "* `bbox_rasterizer_config`\n",
    "* `postprocessing_config`\n",
    "* `training_config`\n",
    "* `cost_function_config`\n",
    "* `evaluation_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a78b7-bbfb-45a3-b4f7-9c96f93a4829",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "We will create the configuration files using templates. Specifically, we have broken the configuration files into separate parts for ease of discussion, which we will combine at the end for the TAO Toolkit to consume. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2b124-5b6e-425b-9233-31c180a34ae3",
   "metadata": {},
   "source": [
    "Execute the below cells to preview the combined training/evaluation configuration file that will be used. It is currently not usable as we have made some intentional modifications that will require correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9386dc00-fc20-40ce-8bad-f3fc2f49fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9d7503-4ac7-49e3-8ea0-3ee5d81caa65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config: {\n",
      "  data_sources: {\n",
      "    tfrecords_path: \"/dli/task/<<<<FIXME>>>>\"\n",
      "    image_directory_path: \"/dli/task/<<<<FIXME>>>>\"\n",
      "  }\n",
      "  image_extension: \"<<<<FIXME>>>>\"\n",
      "  target_class_mapping: {\n",
      "      key: \"<<<<FIXME>>>>\"\n",
      "      value: \"<<<<FIXME>>>>\"\n",
      "  }\n",
      "  validation_fold: 0\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "augmentation_config: {\n",
      "  preprocessing: {\n",
      "    output_image_width: <<<<FIXME>>>>\n",
      "    output_image_height: <<<<FIXME>>>>\n",
      "    output_image_channel: <<<<FIXME>>>>\n",
      "    min_bbox_width: 1.0\n",
      "    min_bbox_height: 1.0\n",
      "  }\n",
      "  spatial_augmentation: {\n",
      "    hflip_probability: 0.5\n",
      "    vflip_probability: 0.5\n",
      "    zoom_min: 1.0\n",
      "    zoom_max: 1.0\n",
      "    translate_max_x: 8.0\n",
      "    translate_max_y: 8.0\n",
      "  }\n",
      "  color_augmentation: {\n",
      "    color_shift_stddev: 0.0\n",
      "    hue_rotation_max: 25.0\n",
      "    saturation_shift_max: 0.2\n",
      "    contrast_scale_max: 0.1\n",
      "    contrast_center: 0.5\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "model_config: {\n",
      "  arch: \"resnet\"\n",
      "  pretrained_model_file: \"/dli/task/<<<<FIXME>>>>\"\n",
      "  freeze_blocks: 0\n",
      "  freeze_blocks: 1\n",
      "  num_layers: 18\n",
      "  use_pooling: false\n",
      "  use_batch_norm: true\n",
      "  dropout_rate: 0.0\n",
      "  objective_set: {\n",
      "    cov: {}\n",
      "    bbox: {\n",
      "      scale: 35.0\n",
      "      offset: 0.5\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "bbox_rasterizer_config: {\n",
      "  target_class_config: {\n",
      "    key: \"<<<<FIXME>>>>\"\n",
      "    value: {\n",
      "      cov_center_x: 0.5\n",
      "      cov_center_y: 0.5\n",
      "      cov_radius_x: 0.4\n",
      "      cov_radius_y: 0.4\n",
      "      bbox_min_radius: 1.0\n",
      "    }\n",
      "  }\n",
      "  deadzone_radius: 0.4\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "postprocessing_config: {\n",
      "  target_class_config: {\n",
      "    key: \"<<<<FIXME>>>>\"\n",
      "    value: {\n",
      "      clustering_config: {\n",
      "        coverage_threshold: 0.005\n",
      "        dbscan_eps: 0.15\n",
      "        dbscan_min_samples: 0.05\n",
      "        minimum_bounding_box_height: 20\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "training_config: {\n",
      "  batch_size_per_gpu: 16\n",
      "  num_epochs: <<<<FIXME>>>>\n",
      "  learning_rate: {\n",
      "    soft_start_annealing_schedule: {\n",
      "      min_learning_rate: 5e-6\n",
      "      max_learning_rate: 5e-4\n",
      "      soft_start: 0.1\n",
      "      annealing: 0.7\n",
      "    }\n",
      "  }\n",
      "  regularizer: {\n",
      "    type: L1\n",
      "    weight: 3e-9\n",
      "  }\n",
      "  optimizer: {\n",
      "    adam: {\n",
      "      epsilon: 1e-08\n",
      "      beta1: 0.9\n",
      "      beta2: 0.999\n",
      "    }\n",
      "  }\n",
      "  cost_scaling: {\n",
      "    enabled: false\n",
      "    initial_exponent: 20.0\n",
      "    increment: 0.005\n",
      "    decrement: 1.0\n",
      "  }\n",
      "  checkpoint_interval: <<<<FIXME>>>>\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "cost_function_config: {\n",
      "  target_classes: {\n",
      "    name: \"car\"\n",
      "    class_weight: 1.0\n",
      "    coverage_foreground_weight: 0.05\n",
      "    objectives: {\n",
      "      name: \"cov\"\n",
      "      initial_weight: 1.0\n",
      "      weight_target: 1.0\n",
      "    }\n",
      "    objectives {\n",
      "      name: \"bbox\"\n",
      "      initial_weight: 10.0\n",
      "      weight_target: 10.0\n",
      "    }\n",
      "  }\n",
      "  enable_autoweighting: true\n",
      "  max_objective_weight: 0.9999\n",
      "  min_objective_weight: 0.0001\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "evaluation_config: {\n",
      "  average_precision_mode: INTEGRATE\n",
      "  validation_period_during_training: <<<<FIXME>>>>\n",
      "  first_validation_epoch: 1\n",
      "  minimum_detection_ground_truth_overlap: {\n",
      "    key: \"<<<<FIXME>>>>\"\n",
      "    value: 0.7\n",
      "  }\n",
      "  evaluation_box_config: {\n",
      "    key: \"<<<<FIXME>>>>\"\n",
      "    value: {\n",
      "      minimum_height: 4\n",
      "      maximum_height: 9999\n",
      "      minimum_width: 4\n",
      "      maximum_width: 9999\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e796308-689a-4db8-93b4-27bc79ad6838",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Modify Data Set Config ####\n",
    "The dataloader defines the path to the data to be trained on and the class mapping for the classes in the data set. We have previously generate TFRecords for the training data sets. To use the newly generated TFRecords, update the `dataset_config` parameter in the spec file to reference the correct directory. Another parameter to consider is `validation_fold`, which we can use `0` to indicate _random data split_. For sequence-wise split, we can use any fold generated from the dataset convert tool. \n",
    "* `data_sources (dict)`: Captures the path to TFRecords to train on. \n",
    "    * `tfrecords_path (str)`: Path to the training TFRecords root/TFRecords_name*, i.e. **/data/tfrecords/kitti_trainval/***.\n",
    "    * `image_directory_path (str)`: Path to the training data source from which the TFRecords was generated. \n",
    "* `image_extension (str)`: Extension of the images to be used without the `.`. \n",
    "* `target_class_mapping (dict)`: This parameter maps the class names in the TFRecords to the target class to be trained. This field enables grouping similar class objects under one umbrella. \n",
    "    * `key (str)`: Value of the class name in the TFRecords file. This must be identical to the one shown in the data set converter log. \n",
    "    * `value (str)`: Corresponds to the value that the network is expected to learn. \n",
    "* `validation_fold (int)`: In case of an n-fold TFRecords, you define the index of the fold to use for validation. For _random split_ partitioning, force the validation fold index to 0 as the TFRecord is just 2-fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69457b8-aa9e-472e-9bd2-f739a6d954ed",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `dataset_config`[(here)](spec_files/dataset_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe63ca6-8489-43b4-b6f3-2e2f3d0c34c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config: {\n",
      "    tfrecords_path: \"/dli/task/tao_project/data/tfrecords/kitti_trainval/*    image_directory_path: \"/dli/task/tao_project/data/training\"\n",
      "   }\n",
      "   image_extension: \"png\"\n",
      "   target_class_mapping: {\n",
      "       key: \"car\"\n",
      "       value: \"car\"\n",
      "  }\n",
      "   validation_fold: 0\n",
      " }"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4de3e7-6674-428b-8a14-5e892f87ee07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_config: {\n",
    "#   data_sources: {\n",
    "#     tfrecords_path: \"/dli/task/tao_project/data/tfrecords/kitti_trainval/*\"\n",
    "#     image_directory_path: \"/dli/task/tao_project/data/training\"\n",
    "#   }\n",
    "#   image_extension: \"png\"\n",
    "#   target_class_mapping: {\n",
    "#       key: \"car\"\n",
    "#       value: \"car\"\n",
    "#   }\n",
    "#   validation_fold: 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d668e8-2f79-4a69-abe5-4ea29b7d87a8",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46b97a-ffdd-4e1e-990c-dac046ae2716",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Modify Augmentation Config ####\n",
    "When using one’s own data set to train and fine-tune a model, the data set can be augmented while training to introduce variations in the data set. This is known as **online augmentation**. This is very useful in training as data variation improves the overall quality of the model and prevents [overfitting](https://en.wikipedia.org/wiki/Overfitting). Training a deep neural network requires large amounts of annotated data, which can be a manual and expensive process. Furthermore, it can be difficult to estimate all the corner cases that the network may go through. The TAO Toolkit provides _spatial augmentation_ (rotate, resize, translate, shear, and flip), _color space augmentation_ (hue rotation, brightness offset, and contrast shift), and _image blur_ to create synthetic data variations. \n",
    "\n",
    "For some of the values, we will need to review the [model card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet). \n",
    "\n",
    "<p><img src='images/model_card_tao.png' width=720></p>\n",
    "\n",
    "* `preprocessing (dict)`: Configures the input image and ground truth label pre-processing module. \n",
    "    * `output_image_width (int)`: This is the same as the width of the network input. \n",
    "    * `output_image_height (int)`: This is the same as the height of the network input. \n",
    "    * `output_image_channel (int)`: This is the same as the channel depth of the network input _(1, 3)_. \n",
    "    * `min_bbox_width (float)`: Minimum width of the object labels to be considered for training. \n",
    "    * `min_bbox_height (float)`: Minimum height of the object labels to be considered for training. \n",
    "* `spatial_augmentation (dict)`: Supports spatial augmentation such as flip, zoom, and translate. \n",
    "    * `hflip_probability (float)`: Probability to flip an input image horizontally _(0.0 - 1.0)_. \n",
    "    * `vflip_probability (float)`: Probability to flip an input image vertically _(0.0 - 1.0)_. \n",
    "    * `zoom_min (float)`: Minimum zoom scale of the input image _(> 0.0)_. \n",
    "    * `zoom_max (float)`: Maximum zoom scale of the input image _(> 0.0)_. \n",
    "    * `translate_max_x (int)`: Maximum translation to be added across the x-axis _(0.0 - output_image_width)_. \n",
    "    * `translate_max_y (int)`: Maximum translation to be added across the y-axis _(0.0 - output_image_height)_. \n",
    "    * `rotate_rad_max (float)`: Angle of rotation to be applied to the images and the training labels _(> 0.0)_. \n",
    "* `color_augmentation (dict)`: Configures the color space transformation. \n",
    "    * `color_shift_stddev (float)`: Standard deviation value for the color shift _(0.0 - 1.0)_. \n",
    "    * `hue_rotation_max (float)`: Maximum rotation angle for the hue rotation matrix _(0.0 - 360.0)_. \n",
    "    * `saturation_shift_max (float)`: Maximum shift that changes the saturation _(0.0 - 1.0)_. \n",
    "    * `contrast_scale_max (float)`: Slope of the contrat as rotated around the provided center _(0.0 - 1.0)_. \n",
    "    * `contrast_center (float)`: Center around which the contrast is rotated _(set to 0.5)_. \n",
    "\n",
    "**NOTE**: If the output image height and output image width of the preprocessing block don’t match with that of the input images mentioned while generating the TFRecords, the images will be either randomly padded or cropped to fit the input resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1602409-fbc5-4498-b8e1-ecd38db747e5",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `augmentation_config`[(here)](spec_files/augmentation_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c968b92-aad8-4617-bcd1-ccedd2943164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " augmentation_config: {\n",
      "   preprocessing: {\n",
      "     output_image_width: 960\n",
      "     output_image_height: 544\n",
      "     output_image_channel: 3\n",
      "     min_bbox_width: 1.0\n",
      "     min_bbox_height: 1.0\n",
      "   }\n",
      "   spatial_augmentation: {\n",
      "     hflip_probability: 0.5\n",
      "     vflip_probability: 0.5\n",
      "     zoom_min: 1.0\n",
      "     zoom_max: 1.0\n",
      "     translate_max_x: 8.0\n",
      "     translate_max_y: 8.0\n",
      "   }\n",
      "   color_augmentation: {\n",
      "     color_shift_stddev: 0.0\n",
      "     hue_rotation_max: 25.0\n",
      "     saturation_shift_max: 0.2\n",
      "     contrast_scale_max: 0.1\n",
      "     contrast_center: 0.5\n",
      "   }\n",
      " }"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/augmentation_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d4678-838f-4ba9-aa77-20d300de306c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# augmentation_config: {\n",
    "#   preprocessing: {\n",
    "#     output_image_width: 960\n",
    "#     output_image_height: 544\n",
    "#     output_image_channel: 3\n",
    "#     min_bbox_width: 1.0\n",
    "#     min_bbox_height: 1.0\n",
    "#   }\n",
    "#   spatial_augmentation: {\n",
    "#     hflip_probability: 0.5\n",
    "#     vflip_probability: 0.5\n",
    "#     zoom_min: 1.0\n",
    "#     zoom_max: 1.0\n",
    "#     translate_max_x: 8.0\n",
    "#     translate_max_y: 8.0\n",
    "#   }\n",
    "#   color_augmentation: {\n",
    "#     color_shift_stddev: 0.0\n",
    "#     hue_rotation_max: 25.0\n",
    "#     saturation_shift_max: 0.2\n",
    "#     contrast_scale_max: 0.1\n",
    "#     contrast_center: 0.5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce5c51-2573-4131-bb1c-0bd63fad1327",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca5d5e-cac7-4be6-afc3-6f7a34511419",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='e3'></a>\n",
    "#### Exercise #3 - Modify Model Config ####\n",
    "The core Object Detection model can be configured using the `model_config` option in the spec file. \n",
    "* `arch (str)`: Architecture of the backbone feature extractor _(default=\"resnet\")_. \n",
    "* `pretrained_model_file (str)`: Path to the pre-trained TAO model file. \n",
    "* `num_layers (int)`: Depth of the feature extractor for scalable templates. \n",
    "* `use_pooling (bool)`: Choose bteween using strided convolutions or MaxPooling while downsampling. We recommend setting this to `false` and using strided convolution. \n",
    "* `objective_set (dict)`: Objective for training the network. For Object Detection networks, set it to learn `cov` and `bbox`. These should not be altered. \n",
    "    * `bbox`\n",
    "        * `scale`: `35.0`\n",
    "        * `offset`: `0.5`\n",
    "    * `cov`\n",
    "* `dropout_rate (float)`: Probability for drop out _(0.0 - 1.0)_. \n",
    "* `load_graph (bool)`: Flag to determine whether or not to load the graph from the pretrained model, or just the weight. For a pruned model, set this parameter to `true` since both the model graph and the weights need to be imported. \n",
    "* `freeze_blocks (float)`: Defines which blocks may be frozen from the instantiated feature extractor template. Freezing a layer in the context of neural networks is about controlling if the weights are updated. When a layer is frozen, such as those in the feature extractor, it means that the weights cannot be modified further. This technique is used to cut down on the computational time for training. More information about Freeze Layers can be found in [TensorFlow's Transfer Learning and Fine-tuning Guide](https://www.tensorflow.org/guide/keras/transfer_learning#freezing_layers_understanding_the_trainable_attribute). \n",
    "* `use_batch_norm (bool)`: Flag to determine whether to use Batch Normalization layers or not _(default=false)_. Batch Normalization is a technique for training very deep neural networks like the ones we're using that standardizes the inputs to a layer for each mini-batch. This is to combat **internal covariate shift**, which occurs when the distribution of the inputs to layers deep in the network change after each mini-batch when the weights are updated. Batch Normalization can stabilize the learning process and dramatically reduce the number of training epochs required to train deep networks. You can find more information about Batch Normalization in [TensorFlow's API documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization). \n",
    "* `freeze_bn (bool)`: Determines whether to freeze the Batch Normalization layers in the module during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84e1dd-56bc-46b4-8326-8526061a1caf",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `model_config`[(here)](spec_files/model_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3b022-885a-4c4b-af91-397801039374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/model_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c356227-cf5e-4827-8aec-aeb259826198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_config: {\n",
    "#   arch: \"resnet\"\n",
    "#   pretrained_model_file: \"/dli/task/tao_project/models/trafficcamnet_vunpruned_v1.0/resnet18_trafficcamnet.tlt\"\n",
    "#   freeze_blocks: 0\n",
    "#   freeze_blocks: 1\n",
    "#   num_layers: 18\n",
    "#   use_pooling: false\n",
    "#   use_batch_norm: true\n",
    "#   dropout_rate: 0.0\n",
    "#   objective_set: {\n",
    "#     cov: {}\n",
    "#     bbox: {\n",
    "#       scale: 35.0\n",
    "#       offset: 0.5\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c502c-5d82-4670-9bf4-9bcb14d1022b",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef44879-74a2-4e67-ae8c-3756e9afc860",
   "metadata": {},
   "source": [
    "<a name='e4'></a>\n",
    "#### Exercise #4 - Modify Bbox Rasterizer Config ####\n",
    "The DetectNet_v2 generates 2 tensors: `cov` and `bbox`. The image is divided into 16x16 grid cells. The `cov` (_coverage_) tensor defines the number of grid cells that are covered by an object. The `bbox` tensor defines the normalized image coordinates of the object top left and bottom right with respect to the grid cell. \n",
    "* `deadzone radius (float)`: Area to be considered dormant around the ellipse of an object. This is useful in cases of overlapping objects so that foreground objects and background objects are not confused _(0.0 - 1.0)_. \n",
    "* `target_class_config (dict)`: Defines the coverage region for an object of a given class and is repeated for each class. \n",
    "    * `cov_center_x (float)`: x-coordinate of the center of the object _(0.0 - 1.0)_. \n",
    "    * `cov_center_y (float)`: y-coordinate of the center of the object _(0.0 - 1.0)_. \n",
    "    * `cov_radius_x (float)`: x-radius of the coverage ellipse _(0.0 - 1.0)_. \n",
    "    * `cov_radius_y (float)`: y-radius of the coverage ellipse _(0.0 - 1.0)_. \n",
    "    * `bbox_min_radius (float)`: Minimum radius of the coverage region to be drawn for boxes _(0.0 - 1.0)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a58821-138b-43f8-87e9-6091af651dc1",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `bbox_rasterizer_config`[(here)](spec_files/bbox_rasterizer_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaeae88-3725-41b4-b71f-d77181b13ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/bbox_rasterizer_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc03c8e-57f5-4f19-b697-cfa2961ac3b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bbox_rasterizer_config: {\n",
    "#   target_class_config: {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       cov_center_x: 0.5\n",
    "#       cov_center_y: 0.5\n",
    "#       cov_radius_x: 0.4\n",
    "#       cov_radius_y: 0.4\n",
    "#       bbox_min_radius: 1.0\n",
    "#     }\n",
    "#   }\n",
    "#   deadzone_radius: 0.4\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a5a15-47e9-429c-894e-a6170110c7fd",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16faa94c-ca78-478e-99de-280806e658e0",
   "metadata": {},
   "source": [
    "<a name='e5'></a>\n",
    "#### Exercise #5 - Modify Post-Processing Config ####\n",
    "The post-processor module generates renderable bounding boxes from the raw detection output. The process includes: \n",
    "1. Filtering out valid detections by thresholding objects using the confidence value in the coverage tensor. \n",
    "2. Clustering the raw filtered predictions using DBSCAN to produce the final rendered bounding boxes. \n",
    "3. Filtering out weaker clusters based on the final confidence threshold derived from the candidate boxes that get grouped into a cluster.\n",
    "* `target_class_config (dict)`: For each class that is being trained for, the `postprocessing_config` has a `target_class_config` element that defines the clustering parameters for the class. \n",
    "    * `key (str)`: Name of the class. \n",
    "    * `value (dict)`: Clustering-config proto parameter that configures the post-processor module. \n",
    "        * `cluster_config (dict)`: \n",
    "            * `coverage_threshold (float)`: Minimum threshold of the coverage tensor output to be considered a valid candidate box for clustering _(0.0 - 1.0)_. \n",
    "            * `dbscan_eps (float)`: Maximum distance between two samples for one to be considered in the neighborhood of the other. The greater the dbscan_eps, the more boxes are grouped together _(0.0 - 1.0)_. \n",
    "            * `dbscan_min_samples (float)`: The total weight in a neighborhood for a point to be considered as a core point _(0.0 - 1.0)_. \n",
    "            * `minimum_bounding_box_height (int)`: Minimum height in pixels to consider as a valid detection post clustering _(0 - input image height)_. \n",
    "            * `clustering_algorithm (enum)`: Defines the post-processing algorithm (DBSCAN i.e. Density Based Spatial Clustering of Application, NMS i.e. Non-Maximal Suppression, or HYBRID) to cluster raw detections to the final bbox render (default=DBSCAN). When using HYBRID mode, ensure both DBSCAN and NMS configuration parameters are defined. \n",
    "            * `DBSCAN` - `dbscan_confidence_threshold (float)`: Confidence threshold used to filter out clustered bounding box output from DBSCAN _(> 0.0, default=0.1)_. \n",
    "            * `NMS` - `nms_iou_threshold (float)`: Intersection Over Union threshold to filter out redundant boxes from raw detection to form final clustered outputs _((0.0 - 1.0), default=0.2)_. \n",
    "            * `NMS` - `nms_confidence_threshold (float)`: Confidence threshold to filter out clustered bounding boxes from NMS _(0.0 - 1.0, default=0)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f54985-f042-4b15-b9e9-6db84f53fc73",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `postprocessing_config`[(here)](spec_files/postprocessing_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f53439-a7b7-4b98-8fcb-7255d3f0e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/postprocessing_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8389bba-5259-493a-af4d-2418c91682df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# postprocessing_config: {\n",
    "#   target_class_config: {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       clustering_config: {\n",
    "#         coverage_threshold: 0.005\n",
    "#         dbscan_eps: 0.15\n",
    "#         dbscan_min_samples: 0.05\n",
    "#         minimum_bounding_box_height: 20\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476808c-457c-423a-a75e-167d37537a9c",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab27d0b-71a8-422b-8e41-0f5c81789999",
   "metadata": {},
   "source": [
    "<a name='e6'></a>\n",
    "#### Exercise #6 - Modify Training Config ####\n",
    "The `training_config` describes the training and learning process. \n",
    "* `batch_size_per_gpu (int)`: Number of images per batch per GPU _(> 1)_. \n",
    "* `num_epochs (int)`: Total number of epochs to run the experiment. \n",
    "* `learning_rate (dict)`: Define the learning rate schedule. Currently DetectNet_v2 only supports: \n",
    "    * `soft_start_annealing_schedule (dict)`, which is configured using: \n",
    "        * `soft_start (float)`: Time to ramp up the learning rate from minimum learning rate to maximum learning rate _(0.0 - 1.0)_. \n",
    "        * `annealing (float`: Time to cool down the learning rate from maximum learning rate to minimum learning rate _(0.0 - 1.0)_. \n",
    "        * `minimum learning rate (float)`: Minimum learning rate in the learning rate schedule _(0.0 - 1.0)_. \n",
    "        * `maximum learning rate (float)`: Maximum learning rate in the learning rate schedule _(0.0 - 1.0)_. \n",
    "* `regularizer (dict)`: Type and weight of the regularizer to be used during training. **It’s a good practice to start with a low regularization weight and gradually fine-tune it to narrow the gap between the training and the validation accuracy. Also, based on our experiments, L1 seems to give us better pruning ratio.**\n",
    "    * `type (enum)`: Supported types are _(NO_REG, L1, L2)_. \n",
    "    * `weight (float)`: Weight of the regularizer. \n",
    "* `optimizer (dict)`: Optimizer to use for training. \n",
    "    * `adam (dict)`\n",
    "        * `epsilon (float)`: Small number to prevent any division by zero in the implementation. \n",
    "        * `beta1 (float)`\n",
    "        * `beta2 (float)`\n",
    "* `cost_scaling (dict)`: Enables cost scaling during training. **Leave this parameter untouched currently for the DetectNet_v2 training pipe**. \n",
    "    * `enabled`: false. \n",
    "    * `initial_exponent`: 20.0. \n",
    "    * `increment`: 0.005. \n",
    "    * `derement`: 1.0. \n",
    "* `checkpoint_interval (int)`: Interval (in epochs) at which `train` saves intermediate models _(0 - num_epochs)_.\n",
    "* `enable_qat (bool)`: Enables model training using Quantization-Aware Training (QAT). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bb4eb-7b55-4e52-b890-509374e1b1cf",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `training_config`[(here)](spec_files/training_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. Pick a low (recommend 10) `num_epochs` to start as the higher the number of epochs the longer the model trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f839e-d23b-4337-a853-7565039612a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/training_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7c7af-8a93-46d7-8ea9-1429b0c6a095",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_config: {\n",
    "#   batch_size_per_gpu: 16\n",
    "#   num_epochs: 10\n",
    "#   learning_rate: {\n",
    "#     soft_start_annealing_schedule: {\n",
    "#       min_learning_rate: 5e-6\n",
    "#       max_learning_rate: 5e-4\n",
    "#       soft_start: 0.1\n",
    "#       annealing: 0.7\n",
    "#     }\n",
    "#   }\n",
    "#   regularizer: {\n",
    "#     type: L1\n",
    "#     weight: 3e-9\n",
    "#   }\n",
    "#   optimizer: {\n",
    "#     adam: {\n",
    "#       epsilon: 1e-08\n",
    "#       beta1: 0.9\n",
    "#       beta2: 0.999\n",
    "#     }\n",
    "#   }\n",
    "#   cost_scaling: {\n",
    "#     enabled: false\n",
    "#     initial_exponent: 20.0\n",
    "#     increment: 0.005\n",
    "#     decrement: 1.0\n",
    "#   }\n",
    "#   checkpoint_interval: 5\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e91b3-ccb9-4a13-ac27-66a003a7a738",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f23b4-fff4-406c-bfa5-eca8f9df5b41",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### Cost Function Config ###\n",
    "The cost function describes how each class is being trained. For best performance, we don't need to change the the parameters within the spec file besides making sure there is an entry for each of the target classes. The other parameters here should remain unchanged. Below is a sample configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205d3cf-e1cf-408b-b5f7-631764131bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/cost_function_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d398e-5474-4b3a-9a3b-a297973e8c22",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='e7'></a>\n",
    "#### Exercise #7 - Modify Evaluation Config ####\n",
    "The evaluator can be configured using the `evaluation_config` parameters. \n",
    "* `average_precision_mode (enum)`: Model _(SAMPLE or INTEGRATE)_ in which the average precision is calculated. \n",
    "* `validation_period_during_training (int)`: Interval at which evaluation is run during training _(1 - total number of epochs)_. \n",
    "* `first_validation_epoch (int)`: First epoch to start running validation _(1 - total number of epochs)_. \n",
    "* `minimum_detection_ground_truth_overlap (dict)`: Minimum Intersection Over Union between ground truth and predicted box after clustering to call a valid detection. \n",
    "    * `key (str)`: Class name. \n",
    "    * `value (float)`: Intersection Over Union value. \n",
    "* `evaluation_box_config (dict)`: Configures the min and max box dimensions to be considered as a valid ground truth and prediction for average precision calculation. \n",
    "    * `minimum_height (float)`: Minimum height in pixels for a valid ground turth and prediction bbox _(0.0 - model image height)_. \n",
    "    * `minimum_width (float)`: Minimum width in pixels for a valid ground turth and prediction bbox _(0.0 - model image width)_. \n",
    "    * `maximum_height (float)`: Maximum height in pixels for a valid ground turth and prediction bbox _(minimum_height - model image height)_. \n",
    "    * `maximum_width (float)`: Maximum width in pixels for a valid ground turth and prediction bbox _(minimum_width - model image width)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0204f62-8e6d-4bb4-ad39-c5f79b261dcc",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `evaluation_config`[(here)](spec_files/evaluation_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b1c26-fefe-4631-a0ad-767c8e55e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/evaluation_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb449f-f02d-4b34-a43f-8742eaa8afaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation_config: {\n",
    "#   average_precision_mode: INTEGRATE\n",
    "#   validation_period_during_training: 5\n",
    "#   first_validation_epoch: 1\n",
    "#   minimum_detection_ground_truth_overlap: {\n",
    "#     key: \"car\"\n",
    "#     value: 0.7\n",
    "#   }\n",
    "#   evaluation_box_config {\n",
    "#     key: \"car\"\n",
    "#     value: {\n",
    "#       minimum_height: 4\n",
    "#       maximum_height: 9999\n",
    "#       minimum_width: 4\n",
    "#       maximum_width: 9999\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c6c6e-5907-4aa4-9418-607fc763c374",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484eb73-a9b4-436b-9a21-6375a89d3a7c",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### Initiate Model Training ###\n",
    "When using the `train` subtask, the `-e` argument indicates the path to the spec file, the `-r` argument indicates the result directory, the `-k` indicates the key to _load_ the pre-trained weights, the `-n` indicates the name of the final step model saved. \n",
    "\n",
    "**Note**: The training may take hours to complete. Detectnet_v2 supports restarting from checkpoints incase the training job is killed prematurely. Training from the closest checkpoint may be resumed by simply re-running the **same** command line. \n",
    "\n",
    "_Multi-GPU support can be enabled for those with the hardware using the `--gpus` argument. When running the training with more than one GPU, we will need to modify the `batch_size_per_gpu` and `learning_rate` to get similar mAP as a 1 GPU training run. In most cases, scaling down the batch-size by a factor of NUM_GPU's or scaling up the learning rate by a factor of NUM_GPU's would be a good place to start._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383f257-f83b-47e2-b10a-699059786cef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a63a82-9e8b-42b1-a5ef-07dccb5e72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View train usage\n",
    "!detectnet_v2 train --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b42c75-5f25-4c74-b4e5-2f4f4f558d2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Initiate the training process\n",
    "!detectnet_v2 train -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                    -r $MODELS_DIR/resnet18_detector \\\n",
    "                    -k tlt_encode \\\n",
    "                    -n resnet18_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31fac3-a7fc-4d25-986e-e2d1a4210c17",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Evaluating the Model ##\n",
    "The model will be evaluated for its performance at the end of training and at specific intervals, which can be configured by using the `evaluation_config` component. The evaluation configuration allows users to select which data set to use for evaluation as well as the evaluation metrics. We can also evaluate the model with the `evaluate` subtask. The `evaluate` subtask runs evaluation on the same validation set that was used during training, but can be updated to include a testing data set in the `dataset_config`. We can also run evaluation on an earlier model by editting the spec file to point to the intended model. When using the `evluate` subtask, the `-e` argument indicates the path to the spec file, the `-m` argument indicates the path to the model, and the `-k` argument indicates the key to _load_ the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615059b8-99b4-4286-9bfb-2466c3e22bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View saved weights\n",
    "print('Model for Each Epoch:')\n",
    "print('---------------------')\n",
    "\n",
    "!ls -lh $MODELS_DIR/resnet18_detector/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a7a5-828c-4245-8c28-27479d35d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View evaluate usage\n",
    "!detectnet_v2 evaluate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673d92-7ebe-48cd-841e-323481cfccbb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the model using the same validation set as training\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt\\\n",
    "                       -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc27f7f-67b0-41fe-ae8a-795863680ecd",
   "metadata": {},
   "source": [
    "**Observations**: <br>\n",
    "Make note of the mAP of this model, which we trained in a fairly short amount of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9188e-6929-41cc-a478-10076e17639c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3'></a>\n",
    "## Model Inference ##\n",
    "The `infer` subtask may be used to visualize bboxes on a single image or directory of images. This is an optional step but highly recommended before deploying the model. The `infer` subtask generates bbox rendered images in the `output_path/images_annotated` directory and the bbox labels in KITTIE format in the `output_path/labels` directory. The inference could take some time depending on the size of the validation data set. \n",
    "\n",
    "The inference spec file is used to select options to run inference. It consists of two blocks: \n",
    "* inferencer_config\n",
    "* bbox_handler_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f47762-2952-4f6e-a36f-158b6d6b3173",
   "metadata": {},
   "source": [
    "<a name='e8'></a>\n",
    "#### Exercise #8 - Modify Inferencer Config ####\n",
    "The inferencer instantiates the model object and preprocessing pipe. \n",
    "* `inferencer_config (dict)`: \n",
    "    * `target_classes (str)`: Name of the target classes the model should output. This parameter is repeated for multi-class model. The order must be the same as the classes in cost_function_config of the training config file. \n",
    "    * `batch_size (int)`: Number of images per batch of inference. \n",
    "    * `image_height (int)`: Height of the image in pixels the model will infer. \n",
    "    * `image_width (int)`: Width of the image in pixels the model will infer. \n",
    "    * `image_channel (int)`: Number of channels per image. \n",
    "    * `tlt_config (dict)`: Proto config to instantiate a model object. \n",
    "        * `model (str)`: Path to the `.tlt` model file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758df62d-d4af-4c11-832b-7f86b3354c0b",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `inferencer_config`[(here)](spec_files/inferencer_config.txt) section of the inference configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a18c15-c679-4980-9313-302e36520e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/inferencer_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdac52-b7a7-4611-b7d2-dca566fb89ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inferencer_config: {\n",
    "#   target_classes: \"car\"\n",
    "#   image_width: 882\n",
    "#   image_height: 692\n",
    "#   image_channels: 3\n",
    "#   batch_size: 16\n",
    "#   tlt_config: {\n",
    "#     model: \"/dli/task/tao_project/models/resnet18_detector/weights/resnet18_detector.tlt\"\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1271f58-4acb-44ab-a467-2b3335799b76",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052b2fe-85e7-4b35-863a-deb9891670a9",
   "metadata": {},
   "source": [
    "<a name='e9'></a>\n",
    "#### Exercise #9 - Modify BBox Handler Config ####\n",
    "The bbox handler handles the post-processing, rendering of bounding boxes and the serialization to KITTI format output labels. It performs the following steps: \n",
    "1. Thresholding the raw outputs to defines grid cells where the detections may be present per class.\n",
    "2. Reconstructing the image space coordinates from the raw coordinates of the inferencer.\n",
    "3. Clustering the raw thresholded predictions.\n",
    "4. Filtering the clustered predictions per class.\n",
    "5. Rendering the final bounding boxes on the image in its input dimensions and serializing them to KITTI format metadata.\n",
    "* `bbox_handler_config (dict)`: \n",
    "    * `kitti_dump (bool)`: Flag to enable saving the final output predictions per image in KITTI format. \n",
    "    * `disable_overlay (bool)`: Flag to disable bbox rendering per image. \n",
    "    * `overlay_linewidth (int)`: Thickness in pixels of the bbox boundaries. \n",
    "    * `classwise_bbox_handler_config (dict)`: Proto object containing parameters to configure the clustering algorithm as well as the bbox renderer. This is repeated for each class. \n",
    "        * `key (str)`: Name of the class. \n",
    "        * `value (dict)`: Clustering-config proto parameter that configures the post-processor module. \n",
    "            * `confidence_model (str)`: Algorithm _(\"aggregate_cov\" or \"mean_cov\")_ to compute the final confidence of the clustered bboxes. \n",
    "            * `bbox_color (dict)`: RGB channel wise color intensity per box. \n",
    "                * `R (int)`: _(0 - 255)_. \n",
    "                * `G (int)`: _(0 - 255)_. \n",
    "                * `B (int)`: _(0 - 255)_. \n",
    "            * `cluster_config (dict)`: \n",
    "                * `coverage_threshold (float)`: Minimum threshold of the coverage tensor output to be considered a valid candidate box for clustering _(0.0 - 1.0)_. \n",
    "                * `dbscan_eps (float)`: Maximum distance between two samples for one to be considered in the neighborhood of the other. The greater the dbscan_eps, the more boxes are grouped together _(0.0 - 1.0)_. \n",
    "                * `dbscan_min_samples (float)`: The total weight in a neighborhood for a point to be considered as a core point _(0.0 - 1.0)_. \n",
    "                * `minimum_bounding_box_height (int)`: Minimum height in pixels to consider as a valid detection post clustering _(0 - input image height)_. \n",
    "                * `clustering_algorithm (enum)`: Defines the post-processing algorithm (DBSCAN i.e. Density Based Spatial Clustering of Application, NMS i.e. Non-Maximal Suppression, or HYBRID) to cluster raw detections to the final bbox render (default=DBSCAN). When using HYBRID mode, ensure both DBSCAN and NMS configuration parameters are defined. \n",
    "                * `DBSCAN` - `dbscan_confidence_threshold (float)`: Confidence threshold used to filter out clustered bounding box output from DBSCAN _(> 0.0, default=0.1)_. \n",
    "                * `NMS` - `nms_iou_threshold (float)`: Intersection Over Union threshold to filter out redundant boxes from raw detection to form final clustered outputs _((0.0 - 1.0), default=0.2)_. \n",
    "                * `NMS` - `nms_confidence_threshold (float)`: Confidence threshold to filter out clustered bounding boxes from NMS _(0.0 - 1.0, default=0)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee89d0-91fb-4899-9673-c4e5b0c01b6f",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `bbox_handler_config`[(here)](spec_files/bbox_handler_config.txt) section of the inference configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e86325-ef8c-4e25-a5e2-5ebe1d7482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/bbox_handler_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1362e7-938f-40eb-813d-a42efedb5b13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bbox_handler_config: {\n",
    "#   kitti_dump: false\n",
    "#   disable_overlay: false\n",
    "#   overlay_linewidth: 2\n",
    "#   classwise_bbox_handler_config: {\n",
    "#     key:\"car\"\n",
    "#     value: {\n",
    "#       confidence_model: \"aggregate_cov\"\n",
    "#       bbox_color: {\n",
    "#         R: 0\n",
    "#         G: 255\n",
    "#         B: 0\n",
    "#       }\n",
    "#       clustering_config: {\n",
    "#         clustering_algorithm: DBSCAN\n",
    "#         coverage_threshold: 0.005\n",
    "#         dbscan_eps: 0.15\n",
    "#         dbscan_min_samples: 0.05\n",
    "#         minimum_bounding_box_height: 20        \n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7eeb5-1bd7-4a29-844b-d9be389e3e89",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae423643-2315-448c-9820-75e57fb62e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/inferencer_config.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_handler_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_inference_config.txt\n",
    "!cat $SPEC_FILES_DIR/combined_inference_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f8258-d995-475d-9cad-c40b3b7da8c7",
   "metadata": {},
   "source": [
    "When using the `inference` subtask, the `-e` argument indicates the path to the inference spec file, the `i` argument indicates the path to the input images directory, the `-o` argument indicates the path to the output images directory, and the `-k` argument indicates the key to _load_ the model. To run inferences for a single image, simply replace the path to the `-i` argument for the `inference` subtask with the path to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ada339-ec6e-4e52-8ba7-71729b86dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View inference usage\n",
    "!detectnet_v2 inference --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe56a3-f1d2-4901-a29d-ed33633e25b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Perform inference on the validation set\n",
    "!detectnet_v2 inference -e $SPEC_FILES_DIR/combined_inference_config.txt \\\n",
    "                        -o $PROJECT_DIR/tao_infer_testing \\\n",
    "                        -i $DATA_DIR/training/images \\\n",
    "                        -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ab2c1-8f90-4533-b0f0-aeb8d2917698",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### Visualize Inference ###\n",
    "We can write a quick function that will help us sample random inferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4827c-c1b5-4103-99ec-94788791af3c",
   "metadata": {},
   "source": [
    "Execute the below cells to visualize the inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea3479-e3de-4008-999d-4987e09eac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Simple grid visualizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['PROJECT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path)]\n",
    "    for idx, img_path in enumerate(random.sample(a, num_images)):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827f734-3ac9-44fc-9c87-e4d28b2e4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Visualizing the random 12 images\n",
    "OUTPUT_PATH = 'tao_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38513719-f295-44d1-825a-f994ca6fbcbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s4'></a>\n",
    "## Model Export ##\n",
    "The `.tlt` model can be used with the TAO Toolkit for inference but DeepStream does not support using it directly. Instead, the TAO Toolkit uses the `export` subtask to export and prepare a trained model for deploying to DeepStream. \n",
    "\n",
    "There are two options to deploy a model trained by the TAO Toolkit to DeepStream: \n",
    "1. Generate a `.etlt` model, or encrypted TAO file, using `export` and integrate it in the DeepStream application as we have done before. DeepStream will generate a TensorRT engine file and then run the inference. \n",
    "2. Generate a device specific optimized TensorRT engine, which will be a `.trt` or `.engine` file, and integrate it in DeepStream.  \n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "_TensorRT engines use machine-specific optimizations that are unique to each hardware configuration so it should be generated for each inference environment. Typically the model training environment will have more computational resources than the deployment environment. The TensorRT engine file generated using the TAO Toolkit cannot be used for deployment unless the deployment hardware is identical to the training GPU. On the other hand, the `.etlt` file can be used universally across training and deployment hardware._\n",
    "\n",
    "When using the `export` subtask, the `-m` argument indicates the path to the `.tlt` model file to be exported, the `-o` argument indicates the path to save the exported `.etlt` model to, the `-k` argument indicates an **encryption key**, and the `-e` argument indicates the experiment spec file. In addition, the TAO Toolkit supports serializing a template config file for the `Gst-nvinfer` element of DeepStream to consume this model. This config file contains the network specific pre-processing parameters and network graph parameters for parsing the etlt model file - very helpful for reference. It also generates a label file, `labels.txt`, that contains the names of the classes that the model was trained for in the order in which the outputs are generated. To generate the DeepStream template files, simply run the `export` subtask using the `--gen_ds_config` option. <br> Execute the below cells to export the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a603260-ada1-4534-a783-332be1ccf247",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View export usage\n",
    "!detectnet_v2 export --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702a1dc-ab54-437e-8e57-abe854b18c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Removing a pre-existing copy if there has been any.\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_unpruned\n",
    "!mkdir -p $MODELS_DIR/resnet18_detector_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6b3b7-2e3f-4e9d-888c-bb0abb781f14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Exporting .tlt model\n",
    "!detectnet_v2 export -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                     -o $MODELS_DIR/resnet18_detector_unpruned/resnet18_detector.etlt \\\n",
    "                     -k tlt_encode \\\n",
    "                     -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                     --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d19d6-3da7-43ed-b613-568105013218",
   "metadata": {},
   "source": [
    "<a name='s5'></a>\n",
    "## Deploying to DeepStream ##\n",
    "The trained model is ready to be deployed in a DeepStream pipeline. We've been provided with a script that will initiate a DeepStream pipeline based on 3 arguments: <br>\n",
    "`python sample_apps/app_03.py <input video file name> <path to nvinfer configuration file> <output video file name>` <br>\n",
    "It has the below architecture, much like the one we created in the first part of this course. \n",
    "\n",
    "<p><img src='images/deepstream_pipeline.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda48cc-71f5-481d-be2c-f4c759c4d4ed",
   "metadata": {},
   "source": [
    "Execute the below cells to pass the input video through this simple DeepStream pipeline twice, once with the purpose-built TrafficCamNet model as is and another with the custom model. Afterwards, compare the results. <br>\n",
    "**Note**: Feel free to modify some of the training parameters at the end of the course to improve the model if time allows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22c2b8-30f1-4699-80eb-9fa7715bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file using the TrafficCamNet as is\n",
    "!cat $SPEC_FILES_DIR/pgie_config_trafficcamnet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfff33-fdf0-4911-852d-d07d0866aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the DeepStream Pipeline with the Purpose-built Model as is\n",
    "%run sample_apps/app_03.py data/sample_30.h264 spec_files/pgie_config_trafficcamnet.txt output.mp4\n",
    "\n",
    "# Convert the video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output.mp4 output_conv.mp4 -y -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cffd4-0977-4cd5-97a7-17db3b50d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file using the custom model\n",
    "!cat $SPEC_FILES_DIR/pgie_config_resnet18_detector_unpruned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0fe03-7fca-4685-9217-dcd0529468e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the DeepStream Pipeline with the Custom Model\n",
    "%run sample_apps/app_03.py /dli/task/data/sample_30.h264 spec_files/pgie_config_resnet18_detector_unpruned.txt output_resnet18_detector_unpruned.mp4\n",
    "\n",
    "# Convert the video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_resnet18_detector_unpruned.mp4 output_resnet18_detector_unpruned_conv.mp4 -y -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394bd49-378a-4ebb-b787-48e830b50176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Compare results\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "    <video alt=\"input\" width='49%' autoplay>\n",
    "        <source src=\"output_conv.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <video alt=\"output\" width='49%' autoplay>\n",
    "        <source src=\"output_resnet18_detector_unpruned_conv.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d74a7-31b4-4674-8100-8b048741b9f8",
   "metadata": {},
   "source": [
    "**Well Done**! When you're ready, let's move to the [next notebook](./04_optimizing_a_video_AI_application.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88e494",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
