{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7492effa",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e435d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to the TAO Toolkit #\n",
    "The TAO Toolkit, Train Adapt Optimize, is a framework that simplifies the AI/ML model development workflow. It lets developers fine-tune pretrained models with custom data to produce highly accurate computer vision models efficiently, eliminating the need for large training runs and deep AI expertise. In addition, it also enables model optimization for inference performance. \n",
    "\n",
    "<p><img src=\"images/tao_toolkit.png\" width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6e6fc-d435-46db-b0e7-cb9a502f094b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will gain the foundational understanding necessary to use the TAO Toolkit effectively, including: \n",
    "* Video AI Model Training Challenges\n",
    "* What is Transfer Learning\n",
    "* How to Optimize AI Models for Video AI Applications\n",
    "* How to Use the TAO Toolkit CLI\n",
    "* Pre-trained Models Support by the TAO Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbe3bf-73bf-476c-9bea-db337196ad9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Table of Contents**<br>\n",
    "This notebooks covers the below sections: \n",
    "1. [Video AI Model Training Workflow](#s1)\n",
    "    * [Deep Learning Challenges](#s1.1)\n",
    "    * [Transfer Learning](#s1.2)\n",
    "    * [TAO Toolkit for Video AI](#s1.3)\n",
    "2. [Video AI Pre-trained Models Supported](#s2)\n",
    "3. [TAO Toolkit Workflow](#s3)\n",
    "    * [TAO Launcher, CLI (Command Line Interface), and Spec Files](#s3.1)\n",
    "    * [Exercise #1 - Explore TAO Toolkit CLI](#e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69bb6d",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## Video AI Model Training Workflow ## \n",
    "At the heart of a video AI application is one or more deep learning models for extracting insights as such as detecting cars and classifying them. They are tuned and optimized to deliver the right level of accuracy and performance. Building a deep learning model consists of several steps, including collecting large, high-quality data sets, preparing the data, training the model, and optimizing the model for deployment. When we train a neural network model, we leverage its ability to perform automatic feature extraction from raw data and associate them to our target. Deep learning model performance increases when we train with more data, but it's a time consuming and computationally intensive process. Once a model is trained, it can be deployed and used for inference. Given the complex nature of the computation involved, models can be large and become a bottle neck for the video AI application. To ensure that the streaming analytics pipeline are effective, the video AI models have to be efficient without sacrificing on the accuracy.\n",
    "<p><img src='images/video_ai_model_training_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d8e-5127-4a1e-b041-634e60a018e7",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### Deep Learning Challenges ###\n",
    "There are some common challenges related to building deep learning models for video AI applications: \n",
    "* Requires knowledge of one or more deep learning frameworks, such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), or [Caffe](https://caffe.berkeleyvision.org/). \n",
    "* Training accurate deep learning models from scratch requires a large amount of data and acquiring them is a costly process. \n",
    "* Deep learning models require significant effort to fine-tune before it is optimized for inference and production ready. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a96b2-973e-45d8-ba58-994fe5374e88",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### Transfer Learning ###\n",
    "In practice, it is rare and inefficient to initiate the learning task on a network with randomly initialized weights due to factors like data scarcity (inadequate number of training samples) or prolonged training times. One of the most common solutions to overcome this is to use transfer learning. Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where developers use a model trained on one task and re-train to use it on a different task. This works surprisingly well as many of the early layers in a neural network are the same for similar tasks. For example, many of the early layers in a convolutional neural network used for a Computer Vision (CV) model are primarily used to identify outlines, curves, and other features in an image. The network formed by these layers are refered to as the **backbone** of a more complex model. Also known as feature extractors, they take as input the image and extracts the feature map upon which the rest of the network is based. The learned features from these layers can be applied to similar tasks carrying out the same identification in other domains. Transfer learning enables adaptation (fine-tuning) of an existing neural network to a new one, which requires significantly less domain-specific data. In most cases, fine-tuning takes significantly less time (a reduction by x10 factor is common), saving time and resources. As it relates to vision AI, transfer learning is used for scene adaptaion by transferring weights from one application to another, adapting to a new point of view or a camera angle. Transfer learning is also used for adding new classifications. \n",
    "\n",
    "<p><img src='images/transfer_learning.png' width=720></p>\n",
    "\n",
    "More information about transfer learning can be found in this [blog](https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f599bd-e5f9-40ca-9249-6304088569bd",
   "metadata": {},
   "source": [
    "<a name='s1.3'></a>\n",
    "### TAO Toolkit for Video AI ###\n",
    "The TAO Toolkit uses pre-trained models to accelerate the AI development process and reduce costs associated with large scale data collection, labeling, and training models from scratch. Transfer learning with pre-trained models can be used for video AI applications in smart cities, retail, healthcare, industrial inspection and more. The TAO Toolkit offers useful features such as: \n",
    "* Zero-coding approach that requires no AI framework expertise, reducing the barrier of entry for anyone who wants to get started building video AI applications. \n",
    "* Flexible configurations that allow customization to help advance users prototype faster. \n",
    "* Large catalogue of production-ready pre-trained models for common CV tasks that can also be customized with users' own data. \n",
    "* Easy to use interface for model optimization such as pruning and quantization-aware training. \n",
    "* Integration with the DeepStream SDK\n",
    "<p><img src='images/transfer_learning.jpg' width=540></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc59be",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Video AI Pre-trained Models Supported ##\n",
    "Developers, system builders, and software partners building video AI applications and services can bring their own custom data to train with and fine-tune pre-trained models quickly instead of going through significant effort in large data collection and training from scratch. There are two types of pre-trained models that users can start with: **general purpose vision models** and **purpose-built pre-trained models**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b2303-264e-42f2-aa5c-afd38e32c54f",
   "metadata": {},
   "source": [
    "* **General purpose vision models** provide pre-trained weights for popular network architectures to train an image classification model, an object detection model, or a segmentation model. This gives users the flexibility and control to build AI models for any number of applications, from smaller lightweight models for edge deployment to larger models for more complex tasks. They are trained on [Open Images](https://opensource.google/projects/open-images-dataset) data set and provide a much better starting point for training versus training from scratch or starting from random weights. \n",
    "\n",
    "    The TAO Toolkit adapts popular network architectures and backbones to custom data, allowing developers to train, fine tune, prune, and export highly optimized and accurate AI models. When working with TAO, first choose the model architecture to be built, then choose one of the supported backbones. \n",
    "<p><img src='images/tao_matrix.png' width=720></p>\n",
    "\n",
    "    _Note: The pre-trained weights from each feature extraction network merely act as a starting point and may not be used without re-training. In addition, the pre-trained weights are network specific and shouldn't be shared across models that use different architectures._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572943d6-9ae3-48a2-85b1-715c8c826b24",
   "metadata": {},
   "source": [
    "* **Purpose-built pre-trained models** are production-quality models that are built for high accuracy and performance. They are trained on millions of objects for common video AI tasks and provide an excellent starting point for any application in smart city, retail, public safety, healthcre, and others. Purpose-built models are freely available on [NGC](https://ngc.nvidia.com/). For each model, there is a pruned version that can be deployed as is or an unpruned version which can be used to re-train with more data for specific use cases. \n",
    "<p><img src='images/purpose-built_models_table.png' width=720></p>\n",
    "\n",
    "    Find the complete list and details [here](https://docs.nvidia.com/tao/tao-toolkit/text/overview.html#pre-trained-models). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d9ebe-4910-47bf-a704-1f55fad0aaa1",
   "metadata": {},
   "source": [
    "* _No third party pre-trained models are supported by the TAO Toolkit. Only NVIDIA pre-trained models from NGC are currently supported which can be retrained with custom data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42fc11",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "### TAO Toolkit Workflow ###\n",
    "Building video AI systems and applications is hard. And tailoring even a single component to the needs of the enterprise for deployment is even harder. Deployment for a domain-specific application typically requires several cycles of re-training, fine-tuning, and deploying the model until it satisfies the requirements. It typically follows the below steps: \n",
    "\n",
    "0. Configuration\n",
    "1. Download a pre-trained model from NGC\n",
    "2. Prepare the data for training\n",
    "3. Train the model using transfer learning\n",
    "4. Evaluate the model for target predictions\n",
    "5. Optimize the model for inference performance\n",
    "6. Export the model for inference\n",
    "\n",
    "<p><img src='images/tao_toolkit_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52b603",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### TAO Launcher, CLI (Command Line Interface), and Spec Files ###\n",
    "The TAO Toolkit is a zero-coding framework that makes it easy to get started. It uses a **launcher** to pull from NGC registry and instantiate the appropriate TAO container that performs the desired subtasks such as convert data, train, evaluate, or export. Users interact with the launcher with its **Command Line Interface** that is configured using simple [**Protocol Buffer**](https://developers.google.com/protocol-buffers) **specification files** to include parameters such as the data set parameters, model parameters, and optimizer and training hyperparameters. More information about the TAO Toolkit Launcher can be found in the [TAO Docs](https://docs.nvidia.com/tao/tao-toolkit/text/tao_launcher.html#tao-launcher). \n",
    "\n",
    "_Note: The TAO Toolkit comes with a set of reference scripts and configuration specifications with default parameter values that enable developers to kick-start training and fine-tuning. This lowers the bar and enables users without a deep understanding of models, expertise in deep learning, or beginning coding skills to be able to train new models and fine-tune the pretrained ones._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba658a8-0124-44fa-b655-e1093ae9df01",
   "metadata": {},
   "source": [
    "**Getting Started with the TAO Launcher CLI**\n",
    "\n",
    "The tasks can be invoked from the TAO Toolkit Launcher using the following convention on the command-line: `tao <task> <subtask> <args_per_subtask>`, where `<args_per_subtask>` are the arguments required for a given subtask. Once the container is launched, the subtasks are run by the TAO Toolkit containers using the appropriate hardware resources. \n",
    "<p><img src='images/tao_launcher.gif' width=720></p>\n",
    "\n",
    "To see the usage of different functionality that are supported, use the `--help` option. For more information, see the [TAO Toolkit Quick Start Guide](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html). \n",
    "Here is the **sample output**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176afd1c-4326-442e-9021-1828dde70f7f",
   "metadata": {},
   "source": [
    "usage: tao [-h]\n",
    "           {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}\n",
    "           ...\n",
    "\n",
    "Launcher for TAO Toolkit.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "\n",
    "tasks:\n",
    "  {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8939251-1122-4713-9ce3-9bddbaa8cc7c",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "**For the purposes of this course - we will not be using the TAO Launcher. Instead, our environment is set up to emulate working inside of a running TAO Toolkit container already.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e6aea",
   "metadata": {},
   "source": [
    "With the TAO Toolkit, users can train models for object detection, classification, segmentation, optical character recognition, facial landmark estimation, gaze estimation, and more. In TAO's terminology, these would be the **tasks**, which support **subtasks** such as `train`, `prune`, `evaluate`, `export`, etc. Each task/subtask requires different combinations of configuration files to accomodate for different parameters, such as the dataset parameters, model parameters, and optimizer and training hyperparameters. Part of what makes TAO Toolkit so easy to use is that most of those parameters are hidden away in the form of experiment specification files (spec files). They are detailed in the [Getting Started Guide](https://docs.nvidia.com/tao/archive/tlt-10/pdf/Transfer-Learning-Toolkit-Getting-Started-Guide-IVA.pdf) for reference. It's very helpful to have these resources handy when working with the TAO Toolkit. In addition, there are a number of specific tasks that help with handling the launched commands. Below is a list of available options for task. We grayed out the tasks for Conversational AI as they are out of scope for this course. \n",
    "\n",
    "<img src='images/tao_tasks.png' width=740>\n",
    "\n",
    "We can use the `--help` option explore the functionality of different tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a411426-1856-44e2-bb0e-3393d527baf6",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1: Explore TAO Toolkit CLI ####\n",
    "Let's explore some TAO Toolkit tasks. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell, choosing a task from options such as: `[classification, detectnet_v2, mask_rcnn, emotionnet, etc]`, follow by a subtask from options such as: `[calibration_tensorfile, dataset_convert, evaluate, export, inference, prune, train]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf32d9c-0338-4c60-805b-6177451d33a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "usage: detectnet_v2 prune [-h] [--num_processes NUM_PROCESSES] [--gpus GPUS]\n",
      "                          [--gpu_index GPU_INDEX [GPU_INDEX ...]] [--use_amp]\n",
      "                          [--log_file LOG_FILE] -m MODEL -o OUTPUT_FILE -k KEY\n",
      "                          [-n NORMALIZER] [-eq EQUALIZATION_CRITERION]\n",
      "                          [-pg PRUNING_GRANULARITY] [-pth PRUNING_THRESHOLD]\n",
      "                          [-nf MIN_NUM_FILTERS]\n",
      "                          [-el [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]]] [-v]\n",
      "                          {calibration_tensorfile,dataset_convert,evaluate,export,inference,prune,train}\n",
      "                          ...\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --num_processes NUM_PROCESSES, -np NUM_PROCESSES\n",
      "                        The number of horovod child processes to be spawned.\n",
      "                        Default is -1(equal to --gpus).\n",
      "  --gpus GPUS           The number of GPUs to be used for the job.\n",
      "  --gpu_index GPU_INDEX [GPU_INDEX ...]\n",
      "                        The indices of the GPU's to be used.\n",
      "  --use_amp             Flag to enable Auto Mixed Precision.\n",
      "  --log_file LOG_FILE   Path to the output log file.\n",
      "  -m MODEL, --model MODEL\n",
      "                        Path to the target model for pruning\n",
      "  -o OUTPUT_FILE, --output_file OUTPUT_FILE\n",
      "                        Output file path for pruned model\n",
      "  -k KEY, --key KEY     Key to load a .tlt model\n",
      "  -n NORMALIZER, --normalizer NORMALIZER\n",
      "                        `max` to normalize by dividing each norm by the\n",
      "                        maximum norm within a layer; `L2` to normalize by\n",
      "                        dividing by the L2 norm of the vector comprising all\n",
      "                        kernel norms. (default: `max`)\n",
      "  -eq EQUALIZATION_CRITERION, --equalization_criterion EQUALIZATION_CRITERION\n",
      "                        Criteria to equalize the stats of inputs to an element\n",
      "                        wise op layer. Options are [arithmetic_mean,\n",
      "                        geometric_mean, union, intersection]. (default:\n",
      "                        `union`)\n",
      "  -pg PRUNING_GRANULARITY, --pruning_granularity PRUNING_GRANULARITY\n",
      "                        Pruning granularity: number of filters to remove at a\n",
      "                        time. (default:8)\n",
      "  -pth PRUNING_THRESHOLD, --pruning_threshold PRUNING_THRESHOLD\n",
      "                        Threshold to compare normalized norm against\n",
      "                        (default:0.1)\n",
      "  -nf MIN_NUM_FILTERS, --min_num_filters MIN_NUM_FILTERS\n",
      "                        Minimum number of filters to keep per layer.\n",
      "                        (default:16)\n",
      "  -el [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]], --excluded_layers [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]]\n",
      "                        List of excluded_layers. Examples: -i item1 item2\n",
      "  -v, --verbose         Include this flag in command line invocation for\n",
      "                        verbose logs.\n",
      "\n",
      "tasks:\n",
      "  {calibration_tensorfile,dataset_convert,evaluate,export,inference,prune,train}\n"
     ]
    }
   ],
   "source": [
    "# Example: !detectnet_v2 train --help\n",
    "! detectnet_v2 prune --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80833b81",
   "metadata": {},
   "source": [
    "**Well Done**! When you're ready, let's move to the [next notebook](./02_preparation_for_model_training.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028db191",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
