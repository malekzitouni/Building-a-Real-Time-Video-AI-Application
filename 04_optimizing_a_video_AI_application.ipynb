{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acdd711",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b1fb8",
   "metadata": {},
   "source": [
    "# Optimizing a Video AI Application #\n",
    "The effectiveness of a video AI application will largely depend on the inference performance of the video AI model(s). Thus far we have been able to train a video AI model with the TAO Toolkit, but we have not considered the inference performance. This is an important consideration to ensure the DeepStream pipeline runs smoothly and without delays. Furthermore, this will allow the video AI application to be deployed on edge devices that have less computational capabilities. A complete model training workflow includes optimization after the model has been trained to use powerful features such as pruning and quantization before deployment. \n",
    "\n",
    "<img src='images/optimized_pre-trained_model_workflow.png' width=1080>\n",
    "\n",
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to use the TAO Toolkit to optimize a model for inference performance, including: \n",
    "* Building a Multi-source DeepStream Pipeline\n",
    "* Fine-Tuning a Video AI Model for Deployment to DeepStream\n",
    "* Pruning a Trained Detectnet_v2 Model\n",
    "* Using Quantization-Aware Training\n",
    "\n",
    "**Table of Contents** \n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Multi-source DeepStream Pipeline](#s1)\n",
    "    * [Exercise #1 - Build a DeepStream Pipeline with Multiple Sources](#e1)\n",
    "2. [Optimizing Video AI Model for Inference](#s2)\n",
    "    * [Model Pruning](#s2.1)\n",
    "    * [Evaluate Pruned Model](#s2.2)\n",
    "    * [Exercise #2 - Model Comparison](#e2)\n",
    "    * [Retrain Pruned Model with Quantization-Aware Training](#s2.3)\n",
    "    * [Exercise #3 - Convert Pruned Model to QAT and Retrain](#e3)\n",
    "3. [Evaluate Retrained Model](#s3)\n",
    "4. [Export with Calibration Cache](#s4)\n",
    "5. [Deployment to DeepStream](#s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7737d5-9b80-471e-9a31-c59f102b20c8",
   "metadata": {},
   "source": [
    "Execute the below cell to set directories for the TAO Toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78daac3f-841b-4f3e-a454-03e3e2a2d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "!mkdir logs\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331d1bf-f438-4b38-a6f7-628ee96f256f",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## Multi-source DeepStream Pipeline ##\n",
    "The DeepStream SDK enables building a pipeline with multiple input video streams. When there are multiple input sources, each source must have its own decoder and be linked to the `Gst-nvstreammux`. The `Gst-nvstreammux` plugin, referred to as the **muxer**, forms a batch of frames from multiple input sources. When connecting a source to the muxer, a new pad must be requested from the muxer using `get_request_pad()` with the pad template `sink_%u`. The muxer will form a batched buffer with `<batch-size>` frames, which is specified using `set_property()`. If the muxer’s output format and input format are the same, the muxer forwards the frames from that source as a part of the muxer’s output batched buffer. If the resolutions are not the same, the mux scales frames from the input into the batched buffer. The muxer maintains that all frames in the batch have the same resolution when it pushes it downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac59a4-227c-4d0e-a8a4-e43b15c3f1a5",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Build a DeepStream Pipeline with Multiple Sources ####\n",
    "To demonstrate a DeepStream pipeline with multiple inputs, we created a sample application [app_04.py](sample_apps/app_04.py) with the below architecture. This pipeline is very similar to the pipelines we've built so far with a few modifications: \n",
    "1. It takes _one_ video file and uses it for an arbitrary number of file sources (`filesrc`). \n",
    "2. It uses a tiler (`Gst-nvmultistreamtiler`) to composite a 2D tile from batched buffers, which needs to have the `rows`, `columns`, `width`, and `height` properties set. \n",
    "3. It uses the Object Detection model we had built in the previous notebook. \n",
    "4. The probe callback function is attached to the source pad of the tiler. \n",
    "\n",
    "We can run the pipeline by executing the script and passing 4 arguments as: <br> `python sample_apps/app_04.py <path to input h264 video> <path to nvinfer config file> <number of file sources> <name of output file>`. \n",
    "\n",
    "<p><img src='images/multi_input_pipeline.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bea01-881b-4534-a201-c3d5e71f3565",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Review the code for [app_04.py](sample_apps/app_04.py). \n",
    "* Modify the `<FIXME>`s only to create the necessary elements that will connect to the `Gst-nvstreammux`, iteratively based on the arguments passed. Please **save changes** to the file. \n",
    "* Execute the below cells to review the nvinfer config file, run the DeepStream pipeline, and view the `nvdia-smi` log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22173b0-bb9e-4fd5-9a32-2a9e7f2e1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file\n",
    "!cat $SPEC_FILES_DIR/pgie_config_trafficcamnet_retrained.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db69f1c-9204-4886-9a93-f96b1fe9e7c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the app_04.py DeepStream pipeline w/ the custom ResNet18 model\n",
    "!nvidia-smi dmon -i 0 \\\n",
    "                 -s ucmt \\\n",
    "                 -c 20 > '/dli/task/logs/smi.log' & \\\n",
    "python sample_apps/app_04.py /dli/task/data/sample_30.h264 \\\n",
    "                            /dli/task/spec_files/pgie_config_resnet18_detector_unpruned.txt \\\n",
    "                            8 \\\n",
    "                            output_tiled.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2c758-d721-4ff4-b2cd-53121c933f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert the output video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_tiled.mp4 output_tiled_conv.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4e3b4-17cc-489a-8582-197d622f174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Show video\n",
    "from IPython.display import Video\n",
    "Video('output_tiled_conv.mp4', width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942d0b0-572c-49fb-820a-e561b19ae668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the smi.log\n",
    "!cat logs/smi.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedda41-348e-495e-9368-548514ae5e18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(number_sources): \n",
    "#     print('Creating source_bin ', i, end='\\r')\n",
    "#     source=Gst.ElementFactory.make('filesrc', 'file-source_%u'%i)\n",
    "#     source.set_property('location', args[1])\n",
    "#     h264parser=Gst.ElementFactory.make('h264parse', 'h264-parser_%u'%i)\n",
    "#     decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder_%u\"%i)\n",
    "#     pipeline.add(source)\n",
    "#     pipeline.add(h264parser)\n",
    "#     pipeline.add(decoder)\n",
    "#     padname=\"sink_%u\"%i\n",
    "#     source.link(h264parser)\n",
    "#     h264parser.link(decoder)\n",
    "#     decodersrcpad=decoder.get_static_pad(\"src\").link(streammux.get_request_pad(padname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3ebbc-6e68-402f-ad88-0c94e2146349",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6682e-8547-4bf3-9a09-5ba66c3d04ae",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "When we process multiple input streams using our current unpruned model, the DeepStream pipeline begins to suffer in performance. \n",
    "1. At the bottom of the output from the pipeline run, we see that it took a while to run the 24 seconds clip, which is significantly longer than it took for a single input. The pipeline processed less than 30 frames per second, which is what the input streams are taken at. This would result in a significant delay if they were live. See [GStreamer's Design Document on Blocking Probe](https://gstreamer.freedesktop.org/documentation/additional/design/probes.html?gi-language=c#blocking-probes) to find out more about why a delay will occur. \n",
    "2. We also saw in the `nvidia-smi` log that the Streaming Multiprocessor is at very high utilization for the duration of the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7eff4d-bbfd-429f-ba67-9a705f33124c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Optimizing Video Model for Inference ##\n",
    "The TAO Toolkit offers several features to optimize a model for inference performance, including **pruning** and **quantization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a8f0a-9142-4a39-9c14-fea6834ce506",
   "metadata": {},
   "source": [
    "<a name='s2.1'></a>\n",
    "### Model Pruning ###\n",
    "Pruning is one way to fine-tune a model for better inference performance. It is one of the key differentiators for the TAO Toolkit, which involves algorithmically removing neurons from the neural network that do not contribute significantly to the overall accuracy. Pruning reduces the overall size of the model significantly, resulting in a much lower memory footprint and higher inference throughput, which are very important for edge deployment. The model pruning step will inadvertently reduce the accuracy of the model. So after pruning, the next step is to retrain the model on the same data set to recover the lost accuracy. \n",
    "\n",
    "<p><img src='images/pruning.svg' width=540></p>\n",
    "\n",
    "More information about pruning can be found in this [NVIDIA Developer Blog](https://developer.nvidia.com/blog/transfer-learning-toolkit-pruning-intelligent-video-analytics/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e10c6-1233-4655-bbb7-19e8613d6bd1",
   "metadata": {},
   "source": [
    "When using the `prune` subtask, the `-m` argument indictates the path to the pre-trained model, the `-o` argument indictates the path to the output file, and the `-k` argument indictates the key to _load_ the model. Some optional arguments include: \n",
    "* `-eq, --equalization_criterion`: Criteria _(arithmetic_mean, geometric_mean, union (default), and intersection)_ to equalize the states of inputs to an element-wise op layer or depth-wise convolutional layer. This parameter is useful for _ResNets_ and _MobileNets_. \n",
    "* `-pg, --pruning_granularity`: Number of filters to remove at a time _(default=8)_. \n",
    "* `-pth`: Threshold to compare the normalized norm against _(default=0.1)_.\n",
    "* `-nf, --min_num_filters`: Minimum number of filters to keep per layer _(default=16)_. \n",
    "* `-el, --excluded_layers`: List of excluded_layers _(default=[])_. \n",
    "\n",
    "Usually, we just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives smaller model (and thus higher inference speed) but worse accuracy. The threshold to use depends on the data set. A `pth` value of _0.1_ is just a starting point. If the retrain accuracy is good, we can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59b4ad-03f7-43af-a3e3-8482e483fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View prune usage\n",
    "!detectnet_v2 prune --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578fe8a-d984-4083-a5e5-5b1bee28c5eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create a new ResNet model folder and prune the resnet18_detector model\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_pruned\n",
    "!mkdir -p $MODELS_DIR/resnet18_detector_pruned\n",
    "\n",
    "!detectnet_v2 prune -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                    -o $MODELS_DIR/resnet18_detector_pruned/resnet18_detector_pruned.tlt \\\n",
    "                    -k tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3571c-d038-4a10-bd1d-361f4a6f30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List the model and sizes\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector/weights\n",
    "\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b83c96-ff56-4b5e-882b-59f743fb9d73",
   "metadata": {},
   "source": [
    "<a name='s2.22'></a>\n",
    "### Evaluate Pruned Model ###\n",
    "Once the model has been pruned, there can be a decrease in accuracy because some previously useful weights may have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c83d6-1849-421f-a624-801862ddf892",
   "metadata": {},
   "source": [
    "Execute the below cells to compare unpruned model evaluation with that of the pruned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bbcef-7988-44eb-b05a-b51bbd80c3a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the unpruned model\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b1881-d01c-4364-aaa4-fba454b6e795",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the pruned model\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector_pruned/resnet18_detector_pruned.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9212c-dd1d-4c3c-9200-4cd753c2e9fd",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Model Comparison ####\n",
    "**Instructions**: \n",
    "* Study the outputs regarding the size and mean average precision (mAP) of the unpruned and pruned model. \n",
    "* Note down how the two models compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23907aa6-ee89-4ca5-b6d5-c8f4eb73dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WRITE ANSWERS HERE #####\n",
    "#\n",
    "# \n",
    "#\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3cc1b-8ef0-4dce-8b6a-744bd21bf951",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### WRITE ANSWERS HERE #####\n",
    "#\n",
    "# The pruned model is significantly smaller in size but has a lower mean average precision. \n",
    "#\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8377f0e-5c98-440a-92be-12fd0fd6578b",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cef0d-e283-47b5-8918-32e20098dc1c",
   "metadata": {},
   "source": [
    "<a name='s2.3'></a>\n",
    "### Retrain Pruned Model with Quantization-Aware Training ###\n",
    "To regain the accuracy, we recommend to retrain this pruned model over the same data set using the `train` subtask with an updated spec file that points to the newly pruned model as the pre-trained model file. There are several things to consider when retraining: \n",
    "* The `regularizer` option should be turned off in the `training_config` for DetectNet_v2 to recover the accuracy when retraining a pruned model. It can be done by setting the regularizer type to `NO_REG`. All other parameters may be retained in the spec file from the previous training.\n",
    "* The `load_graph` option should be set to `true` in the `model_config` to load the pruned model graph. \n",
    "* If after retraining, the model shows some decrease in mAP, it could be that the originally trained model was pruned a little too much. Please try reducing the pruning threshold (thereby reducing the pruning ratio) and use the new model to retrain.\n",
    "* _Optionally_, DetectNet_v2 supports **Quantization-Aware Training** to help with optmizing the model. \n",
    "\n",
    "Deep neural network (DNN) models, such as those routinely used video AI applications, are typically trained on servers with high-end GPUs available in data centers or private/public clouds. Such systems often use **floating-point 32-bit** arithmetic to take advantage of the wider dynamic range for the weights. After a model is trained, however, it often must be deployed at the edge on hardware that has less computational resources and power budget. Running a DNN inference using the full 32-bit representation is not practical for real-time analysis given the compute, memory, and power constraints of the edge. To help reduce the compute budget, while not compromising on the structure and number of parameters in the model, we can run inference at a lower precision. It is advantageous in many cases to use **8-bit integer numbers** for weights. The challenge is that simply rounding the weights after training may result in a lower accuracy model, especially if the weights have a wide dynamic range. While 8-bit **quantization** is appealing to save compute and memory budgets, it is a lossy process. During quantization, a small range of floating-point numbers are squeezed to a fixed number of information buckets. This results in loss of information. In another words, the minute differences which could originally be resolved using 32-bit representations are now lost because they get quantized to the same bucket in 8-bit representations. This is like the rounding errors that one encounters when representing fractional numbers as integers. To maintain accuracy during inferences at lower precision, it is important to try and mitigate errors arising due to this loss of information with Quantization-Aware Training. QAT is used to train DNNs for lower precision INT8 deployment without compromising on accuracy. It emulates the inference time quantization when training a model that may then be used by downstream inference platforms to generate actual quantized models. The error from quantization weights and tensors to INT8 is modeled during training, allowing the model to adapt and mitigate the error. Technically, during QAT the model constructed in the training graph is modified to: \n",
    "1. Replace existing notes with nodes that support fake quantization of its weights. \n",
    "2. Convert existing activation to ReLU-6 (except the output nodes). \n",
    "3. Add Quantize and De-Quantize (QDQ) nodes to compute the dynamic ranges of the intermediate tensors.  \n",
    "\n",
    "The dynamic ranges computed during training are serialized to a **cache file** that is used at inference. \n",
    "\n",
    "<p><img src='images/qat_training.png' width=720></p>\n",
    "\n",
    "More information about Quantization-Aware Training can be found [here](https://developer.nvidia.com/blog/improving-int8-accuracy-using-quantization-aware-training-and-tao-toolkit/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de5aed-4c9e-49fa-94ba-ad962ee578ef",
   "metadata": {},
   "source": [
    "<a name='e3'></a>\n",
    "#### Exercise #3 - Convert Pruned Model to QAT and Retrain ####\n",
    "Supported models can be converted to QAT models by setting the `enable_qat` parameter in the `training_config` component of the spec file to `true`. When creating a training configuration file for retraining, only the `enable_qat` and `regularizer` from the `training_config` component, and `pretrained_model_file` and `load_graph` from the `model_config` component are updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e36747-905d-4923-ba79-7e560459569e",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `model_config`[(separate qat version here)](spec_files/model_config_qat.txt) section and the `training_config`[(separate qat version here)](spec_files/training_config_qat.txt) of the training configuration file by changing the `<FIXME>`s into acceptable values. Please **save changes** to the files.\n",
    "* Execute the below cells to retrain the pruned model with QAT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb96b-741b-4a24-aabe-d5a3ade58e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/model_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ce272-7130-471d-b88a-d892d8cce51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/training_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6b0da-f9f0-4c11-8506-251c04f77be1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_config {\n",
    "#   arch: \"resnet\"\n",
    "#   pretrained_model_file: \"/dli/task/tao_project/models/resnet18_detector_pruned/resnet18_detector_pruned.tlt\"\n",
    "#   load_graph: true\n",
    "#   freeze_blocks: 0\n",
    "#   freeze_blocks: 1\n",
    "#   num_layers: 18\n",
    "#   use_pooling: false\n",
    "#   use_batch_norm: true\n",
    "#   dropout_rate: 0.0\n",
    "#   objective_set: {\n",
    "#     cov: {}\n",
    "#     bbox: {\n",
    "#       scale: 35.0\n",
    "#       offset: 0.5\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# training_config: {\n",
    "#   batch_size_per_gpu: 16\n",
    "#   num_epochs: 10\n",
    "#   enable_qat: true\n",
    "#   learning_rate: {\n",
    "#     soft_start_annealing_schedule: {\n",
    "#       min_learning_rate: 5e-6\n",
    "#       max_learning_rate: 5e-4\n",
    "#       soft_start: 0.1\n",
    "#       annealing: 0.7\n",
    "#     }\n",
    "#   }\n",
    "#   regularizer: {\n",
    "#     type: L1\n",
    "#     weight: 3e-9\n",
    "#   }\n",
    "#   optimizer: {\n",
    "#     adam: {\n",
    "#       epsilon: 1e-08\n",
    "#       beta1: 0.9\n",
    "#       beta2: 0.999\n",
    "#     }\n",
    "#   }\n",
    "#   cost_scaling: {\n",
    "#     enabled: false\n",
    "#     initial_exponent: 20.0\n",
    "#     increment: 0.005\n",
    "#     decrement: 1.0\n",
    "#   }\n",
    "#   checkpoint_interval: 5\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55925a-cd4f-42da-a20d-59250b5efe06",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff8a43-4108-44f8-8dc6-1be8c260c9c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# UPDATED enable_qat and regularizer from training_config\n",
    "# UPDATED pretrained_model_file and load_graph from model_config\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config_qat.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config_qat.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config_qat.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10c586-3205-44f6-9ce3-d17363b36628",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Initiate the training process\n",
    "!detectnet_v2 train -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                    -r $MODELS_DIR/resnet18_detector_pruned_retrained_qat \\\n",
    "                    -k tlt_encode \\\n",
    "                    -n resnet18_detector_pruned_retrained_qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a8b55-a11a-4b03-b723-1c97dbc42a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List the newly retrained model\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765e20d-9fb7-4a82-ae13-66a40feac5c2",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "### Evaluate Retrained Model ###\n",
    "Once the retraining is complete, we can evaluate the QAT enabled pruned retrained model. The mAP (mean average precision) of this model should be comparable to that of the unpruned model (without QAT). However, due to quantization, it is possible sometimes to see a drop in the mAP value. Pruning and retraining can be an iterative process, but the TAO Toolkit makes it easy to rapidly prototype different versions of the video AI model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643daab1-33c3-4be9-a1af-9dd284c3938a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the model using the same validation set as training\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights/resnet18_detector_pruned_retrained_qat.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839418c3-5129-439b-85c9-59963eab72f2",
   "metadata": {},
   "source": [
    "<a name='s4'></a>\n",
    "## Export Model with Calibration Cache ##\n",
    "When we feel confident in the model's accuracy as well as inference performance, it can be exported to integrate into DeepStream. To enable inference at lower precision for better performance, the **TensorRT engine** needs to be generated in INT8 mode. This process requires an additional **cache file** that contains scale factors to help combat quantization errors, which may arise due to low-precision arithmetic. The calibration cache can optionally be created with the `export` subtask. This is referred to as exporting in **INT8 mode**. When using the `export` subtask, we can include the `--cal_cache_file` argument to indicate the path to save the calibration cache file to and the `--data_type int8` argument to indicate the desired data type. The options for the `--data_type` argument are `fp32`, `fp16`, and `int8`. The default value is `fp32` if inference in INT8 mode is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4536043-cba1-4bfa-b254-34da1e3dd82c",
   "metadata": {},
   "source": [
    "Execute the below cell to export the QAT trained model. This command generates an `.etlt` file from the trained model and serializes the corresponding INT8 scales as a TensorRT readable calibration cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25067f37-32e7-41be-baaa-5eb5d224d69f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Delete duplicate copies\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_final/resnet18_detector_pruned_retrained_qat.etlt\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_final/cal.bin\n",
    "\n",
    "# Export the QAT trained model\n",
    "!detectnet_v2 export -m $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights/resnet18_detector_pruned_retrained_qat.tlt \\\n",
    "                     -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                     -o $MODELS_DIR/resnet18_detector_final/resnet18_detector_pruned_retrained_qat.etlt \\\n",
    "                     -k tlt_encode \\\n",
    "                     --cal_cache_file $MODELS_DIR/resnet18_detector_final/cal.bin \\\n",
    "                     --data_type int8 \\\n",
    "                     --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9a72d-ad8f-411f-a118-99b90505e678",
   "metadata": {},
   "source": [
    "<a name='s5'></a>\n",
    "### Deployment to DeepStream ###\n",
    "The pruned, QAT retrained model is ready to be deployed to DeepStream. We are now able to use `network-mode=1` for INT8 mode in the configuration file for `Gst-nvinfer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268a8da-025f-4db9-9f19-502d4ce1428e",
   "metadata": {},
   "source": [
    "Execute the below cells to read the modified `Gst-nvinfer config file` and pass it to `app_04.py` to run the DeepStream pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e5893-326b-42ee-9add-b851b1a839b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the app_04.py DeepStream pipeline w/ the pruned ResNet18 model\n",
    "!nvidia-smi dmon -i 0 \\\n",
    "                 -s ucmt \\\n",
    "                 -c 20 > '/dli/task/logs/smi.log' & \\\n",
    "python sample_apps/app_04.py /dli/task/data/sample_30.h264 \\\n",
    "                            spec_files/pgie_config_resnet18_detector_optimized.txt \\\n",
    "                            16 \\\n",
    "                            output_tiled_optimized.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836af9cc-fc51-4b2e-9a8a-1160794cedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the smi.log\n",
    "!cat logs/smi.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86274023-8fe8-479e-9a24-4498383baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert the output video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_tiled_optimized.mp4 output_tiled_optimized_conv.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1578b-28bb-42e4-aadb-c1052f739e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Show video\n",
    "Video('output_tiled_optimized_conv.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033a792-e622-4ce3-b3d8-2a4b86b7f3de",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "The pipeline runs smoothly with the pruned model. It is memory and hardware efficient, allowing it to perform accurate, real-time video AI inference from multiple sources without noticeable latency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38589afe",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
